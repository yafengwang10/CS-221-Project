---
title: "CS 221 Project Final R Code"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This R-markdown file contains the R-code for the project report of my CS 221 project.
```{r}
library(caTools)
library(caret)
library(e1071)
library(MASS)
library(dplyr)
library(ggplot2)
library(naniar)
#library(randomForest)
```

This "convert" helper function is due to Brigitte Mueller, [Predicting Heart Disease UCI](https://github.com/mbbrigitte/Predicting_heart_disease_UCI/blob/master/heartdisease_UCI.Rmd). I'm borrowing this helper here to change the data type of the Cleveland data read from the website. 
```{r}
convert = function(obj,types){
    for (i in 1:length(obj)){
        FUN = switch(types[i],character = as.character, 
                                   numeric = as.numeric, 
                                   factor = as.factor)
        obj[,i] = FUN(obj[,i])
    }
    obj
}
```

The heart disease data are directly read from the UCI website. 
```{r}
cleveland.data = read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data",header=FALSE,sep=",",na.strings = '?')
va.data = read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.va.data",header=FALSE,sep=",",na.strings = '?')
switzerland.data = read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data",header=FALSE,sep=",",na.strings = '?')
hungarian.data = read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data",header=FALSE,sep=",",na.strings = '?')
df_list <- list(cleveland.data, va.data, switzerland.data, hungarian.data)
combined.data = Reduce(function(x, y) merge(x, y, all=TRUE), df_list, accumulate=FALSE)
names(combined.data) = c( "age", "sex", "cp", "trestbps", "chol","fbs", "restecg",
                   "thalach","exang", "oldpeak","slope", "ca", "thal", "status")
combined.data$status[combined.data$status > 0] = 1
chclass = c("numeric","factor","factor","numeric","numeric","factor","factor","numeric","factor","numeric","factor","factor","factor","factor")
combined.data = convert(combined.data,chclass)
levels(combined.data$status) = c("healthy", "disease")
levels(combined.data$sex) = c("female", "male")
summary(combined.data)
str(combined.data)
```

Graphing missing data
```{r}
gg_miss_var(combined.data)
```

Processing missing data
```{r}
combined.data = subset(combined.data, select = -c(11, 12, 13))
combined.data = na.omit(combined.data)
```

Graphics for data exploration
```{r}
ggplot(combined.data, aes(x = status)) + geom_bar()

ggplot(combined.data, aes(x = sex, fill=status)) + geom_bar(position="dodge")

ggplot(combined.data, aes(x=age, color = status)) + geom_density()

ggplot(combined.data, aes(x=status, y = age)) + geom_boxplot()

ggplot(combined.data, aes(x = cp, fill=status)) + geom_bar(position="dodge")

ggplot(combined.data, aes(x=trestbps, color = status)) + geom_density()
ggplot(combined.data, aes(x=status, y = trestbps)) + geom_boxplot()

ggplot(combined.data, aes(x=chol, color = status)) + geom_density()
ggplot(combined.data, aes(x=status, y = chol)) + geom_boxplot()

ggplot(combined.data, aes(x = fbs, fill=status)) + geom_bar(position="dodge")

ggplot(combined.data, aes(x = restecg, fill=status)) + geom_bar(position="dodge")

ggplot(combined.data, aes(x=thalach, color = status)) + geom_density()
ggplot(combined.data, aes(x=status, y = thalach)) + geom_boxplot()

ggplot(combined.data, aes(x = exang, fill=status)) + geom_bar(position="dodge")

ggplot(combined.data, aes(x=oldpeak, color = status)) + geom_density()
ggplot(combined.data, aes(x=status, y = oldpeak)) + geom_boxplot()
```

Splitting training and testing data 
```{r}
set.seed(100)
sample = sample.split(combined.data, SplitRatio = 0.70)
train = subset(combined.data, sample == T)
test = subset(combined.data, sample == F)
summary(test)
ggplot(test, aes(x = status)) + geom_bar()
```

Baseline approach: Logistic regression (both with full model training, and with the selected five predictors) 
```{r}
logit.model = train(status ~ ., data=train, method = 'glm', family = 'binomial')
summary(logit.model)
logit.result = predict(logit.model, test)
confusionMatrix(test$status, logit.result)
smalllogit.model = train(status ~ sex +cp + thalach +exang +oldpeak, data=train, method = 'glm', family = 'binomial')
smalllogit.result = predict(smalllogit.model, test)
confusionMatrix(test$status, smalllogit.result)
```

variable importance for logistic model
```{r}
importance = varImp(logit.model, scale=FALSE)
importance
plot(importance)
```

Approach 2: Naive Bayes classification, with both full model, a small model with 5 predictors, and a "best" model that includes all the predictors except trestbps. 
```{r}
naiveBayes.model = naiveBayes(status ~., data = train)
naiveBayes.model
naiveBayes.result = predict(naiveBayes.model, train)
confusionMatrix(train$status, naiveBayes.result)

smallnaiveBayes.model = naiveBayes(status ~sex +cp + thalach +exang +oldpeak, data = train)
smallnaiveBayes.result = predict(smallnaiveBayes.model, test)
confusionMatrix(test$status, smallnaiveBayes.result)

bestnaiveBayes.model = naiveBayes(status ~sex +cp + thalach +exang +oldpeak+restecg+fbs+chol+age, data = train)
bestnaiveBayes.result = predict(bestnaiveBayes.model, test)
confusionMatrix(test$status, bestnaiveBayes.result)
```


Approach 3: Support vector machines, using a 3-repeat, 10-fold cross validation to fine tune the hyperparameters C, using Gaussian radial kernel. Includes both the results of the full model and a small model with the selected 5 predictors. 
```{r}
fitControl = trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(100)
svm.model = train(status ~., data = train, method = "svmRadial", trControl = fitControl)
svm.model
svm.result = predict(svm.model, newdata = test)
confusionMatrix(test$status, svm.result)


set.seed(100)
smallsvm.model = train(status ~sex +cp + thalach +exang +oldpeak, data = train, 
                 method = "svmRadial",
                 trControl = fitControl)
smallsvm.result = predict(smallsvm.model, newdata = test)
confusionMatrix(test$status, smallsvm.result)
```

Variable importance for the full svm model. 
```{r}
importance = varImp(svm.model, scale=FALSE)
importance
plot(importance)
```

Approach 4: Random forests, with 10 fold, 3 repeat cross validation to tune the parameter mtry. With results from both the full model and the small model using the 5 selected predictors
```{r}
fitControl = trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(100)
rf.model = train(status~., data=train, method="rf", trControl = fitControl)
rf.model
rf.result = predict(rf.model, test)
confusionMatrix(test$status, rf.result)
predictors(rf.model)
```

Small random forests model using only the 5 selected predictors.
```{r}
set.seed(100)
fitControl = trainControl(method = "repeatedcv", number = 10, repeats = 3)
smallrf.model = train(status~sex +cp + thalach +exang +oldpeak, data=train, method="rf")
smallrf.model
smallrf.result = predict(smallrf.model, test)
confusionMatrix(test$status, smallrf.result)
```

Variable importance for the full random forest model
```{r}
importance = varImp(rf.model, scale=FALSE)
importance
plot(importance)
```

Variable selection via recursive variable elimination using random forests
```{r}
set.seed(100)
control = rfeControl(functions=rfFuncs, method="cv", number=10, repeats = 3, returnResamp = "all")
results = rfe(train[, 1:10], train[, 11],  rfeControl=control)
results
print(results)
predictors(results)
plot(results, type = c("g", "o"))
```

Approach 5: Single layer neural network, using 10 fold, 3 repeat cross validation to tune the parameters size and decay. 
```{r}
fitControl = trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(100)
nnet.model = train(status ~ ., data = train, method = "nnet",  trControl = fitControl, verbose = FALSE, trace=FALSE)
nnet.model
nnet.result = predict(nnet.model, test)
confusionMatrix(test$status, nnet.result)
```


```{r}
set.seed(100)
smallnnet.model = train(status ~ sex +cp + thalach +exang +oldpeak, data = train, method = "nnet",  trControl = fitControl, verbose = FALSE, trace=FALSE)
smallnnet.model
smallnnet.result = predict(smallnnet.model, test)
confusionMatrix(test$status, smallnnet.result)
```

variable importance based on neural network model 
```{r}
ggplot(nnet.model)
importance = varImp(nnet.model, scale=FALSE)
importance
plot(importance)
```









